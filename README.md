# ğŸ“˜ CreditExplain RAG

**Evidence-backed, auditable explanations for credit decisions â€” with citations, PII-safety, and regulator-ready audit trails.**

Built for the **NSK.AI RAG Hackathon 2025**, CreditExplain RAG helps **compliance officers and analysts** quickly answer:

* *â€œWhy was this loan declined?â€*
* *â€œWhich clause justifies KYC step X?â€*

The system integrates advanced **RAG techniques** (SELF-RAG critic loop, reranking, provenance logging, PII redaction) to provide trustworthy, auditable explanations.

---

## âœ¨ Features

* ğŸ” **Adaptive Retrieval (SELF-RAG):** Critic decides if/when to retrieve; reflection tokens (`ISREL`, `ISSUP`, `ISUSE`) score evidence.
* ğŸ“‘ **Evidence-backed Explanations:** Every answer cites chunks from regulatory docs, T\&Cs, or policies.
* ğŸ›¡ï¸ **PII Redaction:** Personal data filtered out before display.
* ğŸ“Š **Audit & Metrics:** Downloadable audit JSON/PDF with provenance; dashboards for retrieval precision and ISSUP distribution.
* ğŸ’¡ **Follow-up Suggestions:** Sidebar with prioritized next questions.
* ğŸ–¥ï¸ **Simple UI:** Sleek React web app â€” *Ingest Documents* and *Query Agent*.

---

## ğŸ“‚ Repository Layout

```bash
creditexplain/
â”œâ”€â”€ api/                   # FastAPI backend
â”‚   â”œâ”€â”€ main.py            # Entrypoint: defines app, mounts routers
â”‚   â”œâ”€â”€ safety.py          # PII redaction middleware
â”‚   â”œâ”€â”€ dependencies.py    # Common dependencies (e.g., DB, settings)
â”‚
â”œâ”€â”€ core/                  # Orchestrator, RAG loop, prompts
â”‚   â”œâ”€â”€ self_rag.py
â”‚   â”œâ”€â”€ retrieval.py
â”‚   â”œâ”€â”€ prompts.py
â”‚   â””â”€â”€ provenance.py
â”‚
â”œâ”€â”€ ingest/                # Loaders, chunking, embedding pipeline
â”‚   â”œâ”€â”€ loader.py
â”‚   â”œâ”€â”€ chunker.py
â”‚   â”œâ”€â”€ index.py
â”‚   â””â”€â”€ normalize.py
â”‚
â”œâ”€â”€ models/                # Critic & reranker configurations
â”‚   â”œâ”€â”€ critic.py
â”‚   â””â”€â”€ reranker.py
â”‚
â”œâ”€â”€ vectorstore/           # Persisted vector DB (Chroma or FAISS)
â”‚
â”œâ”€â”€ eval/                  # Metrics & audit evaluations
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ runs/
â”‚
â”œâ”€â”€ data/                  # Sample input documents (not versioned)
â”‚
â”œâ”€â”€ diagrams/              # Architecture diagrams
â”‚
â”œâ”€â”€ frontend/              # React + Vite UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ Upload.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Query.tsx
â”‚   â”‚   â”‚   â””â”€â”€ Metrics.tsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ExplanationCard.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ CitationPreview.tsx
â”‚   â”‚   â”‚   â””â”€â”€ SidebarQuestions.tsx
â”‚   â”‚   â””â”€â”€ api/axios.ts
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.js
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â””â”€â”€ .env.example
```

---

## ğŸ› ï¸ Setup

### 1. Clone & install

```bash
git clone https://github.com/<your-org>/creditexplain.git
cd creditexplain
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Configure environment

Copy `.env.example` â†’ `.env` and fill in your keys:

```bash
OPENAI_API_KEY=sk-xxxx
CHROMA_PERSIST_DIR=./vectorstore/chroma
EMBEDDINGS_FALLBACK=all-mpnet-base-v2
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
```

### 3. Ingest documents

```bash
# Place PDFs in data/raw/
make ingest
```

### 4. Run the app

```bash
make run
# React UI starts at http://localhost:8501
```

---

## ğŸš€ Usage

1. Go to **Page A (Ingest Docs)** â€” upload regulatory PDFs or product T\&Cs.
2. Switch to **Page B (Query Agent)** â€” ask queries like:

   * â€œWhy was loan X declined?â€
   * â€œWhich clause justifies KYC Step Y?â€
3. Review:

   * Explanation (with citations)
   * Confidence scores (ISREL/ISSUP/ISUSE)
   * Redacted evidence chunks
   * Suggested follow-up questions
4. Export **Audit JSON/PDF** for compliance traceability.

---

## ğŸ“Š Evaluation & Metrics

Metrics are logged to `eval/runs/` and summarized in dashboards:

* **Retrieval Precision (P\@1, P\@3)**
* **Mean Reciprocal Rank (MRR)** (with vs without reranker)
* **Citation precision**
* **ISSUP distribution** (evidence support)
* **Latency breakdown**

Example run:

```bash
make eval
# Produces eval/runs/demo.json and summary charts
```

---

## ğŸ§‘â€ğŸ¤â€ğŸ§‘ Team & Roles

* **Role A â€“ Data & Ingestion Lead**

  * Ingestion, chunking, embeddings, vector DB
* **Role B â€“ Reasoning Core Lead**

  * SELF-RAG critic, retriever/reranker, generator, provenance
* **Role C â€“ UI & Safety Lead**

  * Streamlit UI, PII redaction, metrics, audit exports

---

## ğŸ¥ Demo & Presentation

* **Demo Video (2â€“3 min):** \[link placeholder]
* **Architecture Diagram:** [`diagrams/creditexplain_infra.png`](./diagrams/creditexplain_infra.png)
* **Data Flow Chart (with examples):** [`diagrams/creditexplain_dataflow.png`](./diagrams/creditexplain_dataflow.png)

---

## ğŸ“œ License

MIT License (see [LICENSE](./LICENSE)).

---
